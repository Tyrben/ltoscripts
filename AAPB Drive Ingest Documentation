This file documents the process of using the AA_PBCorescript_with_checks, AA_LTO_checksum_update, AA_LTO_checksum_second_tape, batch_qt_proofsheet, and proof_check scripts to back up a series of hard drives at WGBH between January and May of 2015.  

Project Narrative Overview

Over the summer of 2014, every file that was characterized as a media file in WGBH’s then-current Artesia digital asset management system was retrieved from WGBH’s SAMFS/QFS storage system and copied onto a local drive, for the purpose of inclusion in the American Archive of Public Broadcasting.  These files eventually filled 82 hard drives.  Each of these drives was sent to Crawford Media Services, the vendor partner on the AAPB project, for transcoding and access through the Archival Management System (AMS) and preservation at the Library of Congress.

After Crawford completed their portion of the project, they mailed the original 82 drives back to the WGBH Media Library and Archive (MLA).  The MLA had by this point discontinued the use of Artesia to store assets on LTO-4 tape through the institutional SAMFS/QFS, and was in the process of developing workflows for backing up digital assets to LTO 6 tapes under the direct control of the MLA.  Rather than attempting to transfer everything out of the old system for a second time, the archives staff decided to consider the files retrieved from the system and placed on these drives as the archival masters of these born-digital assets going forward, and use the ingest and backup of the AAPB drives as a test case for digital preservation at WGBH with the new LTO-6 backup system.  Designing and implementing the workflow of this process has been one of my primary projects as the National Digital Stewardship Resident at WGBH. 

As the project was originally conceived, much of the work of metadata and fixity verification would have been performed through HydraDAM, the open-source digital asset management system planned to replace Artesia at WGBH.  However, as HydraDAM’s implementation was delayed, I’ve developed a series of project-specific scripts designed to accomplish the batch processing of a DAM system and allow the project to move forward.   

For each file, the scripts create a PBCore Instantiation record, which include the original Artesia GUID and the AAPB GUID assigned by Crawford, detailed technical metadata generated by the MediaInfo program, location information, and an MD5 checksum.  This record lives as an XML document alongside the file in digital storage, and is also stored in a central location for later ingest into HydraDAM.  The scripts also automatically generate lists of checksums so the files can be verified after being backed up to LTO tape. 

Scripts and procedures have evolved over time in response to discoveries about the contents of the drives.  Changes to the scripts and the workflow were triggered by the following discoveries:

1. Many drives are filled to capacity – 3 TB of data – which was more than could fit on a single LTO tape; as a result, I expanded the scripts and processes that were originally designed around a 1:1 drive:LTO ratio to accommodate drives that had to be backed up across multiple tapes.
2. The original process of pulling all media files down from Artesia had resulted in a broad collection that included many low-quality .mp4 derivatives of existing high-quality master files, as well as some duplicate files.  I therefore adapted the scripts to verify that files going onto LTO tape were, in fact, unique master files.  
3. Many drives contained failed files, due to technical problems with the original transfer out of Artesia.  Some of these failed files had already been identified as failed by Crawford, and could be isolated by referring to Crawford’s documentation.  Some had not been identified by Crawford, but could be identified as failures by comparing their MD5 checksum value to a checksum value recorded before the file was ingested into Artesia.  Others, however, did not have a pre-existing checksum value, and could only be identified as failed via a manual QC process.  As there was not time or staff available to manually QC every one of the over 10,000 files included on the drives, I updated the scripts to identify the files that could be verified as good or bad by their checksum values, and then added the QC_proofsheet procedure to the workflow to assist with future identification of bad files that could not be automatically checked.   

A complete list of scripts written for the project and their functions is attached to the end of this document as Appendix A.  Many of the scripts refer to specific documents that contain or record information such as GUIDs and MD5 checksums; those documents are listed in Appendix B.  

Originally, the drives were sent off to Crawford in several ‘batches’.   Every drive contains a series of top-level folders named after the Artesia GUID (unique ID) assigned to the digital file; the folders each contain a single media file.  (Filenames are not standardized.)  Batch 1 is a single drive, containing only audio.  Batch 2 is a series of six drives, none of which contain failed files.  Batch 3 is subdivided into seven groups: Batch 3-1, Batch 3-2, Batch 3-3, Batch 3-4, Batch 3-5, and Batch 3-6.   The ten drives in Batch 3-1 contain failed files identified by Crawford; successful versions of these files were later sent back to Crawford on a further series of five “redo” drives.   Subsequent batches contain no pre-identified failed files, but several unidentified failed files.  

The finalized workflow, developed during the processing of Batch 3-2 and carried forward consistently for the remaining 60 drives, looks like this: 

1. Remove drive from container
2. Barcode drive
3. Identify LTO tape for drive
4. Barcode and format LTO tape
5. Generate instantiation documents and checksums for drive
6. Create proofsheet for QC
 7. List files not proofed
8. Copy files and instantiation documents to LTO
9. Generate checksums for LTO
10. Compare checksums to verify successful transfer
11. Document LTO and checksum locations in LTO spreadsheet
12. Transfer drive to vault
13. Transfer LTO to vault
13. Create record for drive
14. Create record for LTO tape

The rest of this document explains each of these steps in more detail, as well as describing the rationales behind some of the decision-making involved.  

Project Workflow Process

1.  Remove drive from container

At the beginning of the project, we decided to remove each drive from its casing and plan for all future access to occur through a drive bay.   There are two major reasons for this:

1. Some of the drives used during the AAPB project had USB 3.0 interfaces, but not eSata interfaces; on the other hand, the Mac desktop primarily used for the project had an eSata interface, but not a USB 3.0.  Data transfer operations would be more efficient if all drives could be accessed through the same eSata connection.   

2. While attached to their original cases, the drives are bulky and take up a significant amount of shelf space in the archives vault.  When removed from their cases and stored as bare drives in plastic anti-static bags, the drives can be stacked in shelves designed for CD cases and take up much less room.  

The two most commonly used drives for the AAPB project were 3.0 Seagate Backup Pluses and Lacie D2 Quadras (which also contain Seagate 3.0 hard drives at their core, but have a superior casing).  

For a demonstration on the process of removing a Seagate Backup Plus drive from its container, please see: https://www.youtube.com/watch?v=dDp9sC3TT0s

For a demonstration on the process of removing a Lacie D2 Quadra drive from its container, please see: https://macbitz.wordpress.com/2011/01/11/upgrading-the-disk-in-a-lacie-d2-quadra/

NOTES: Over the course of the project, we discovered that some of the Seagate Backup Plus drives were malfunctioning when removed from their container, but behaved properly when placed back inside the case in which they had originally been stored.  As a result, a series of twelve drives were not removed from their cases, and are stored in their cases and original boxes.  These drives include:

aapb_001b63a401f3_20140131172021, barcode 363958
aapb_001b63a401f3_20140131172008, barcode 363959
aapb_109add5b4591_20140218112334, barcode 363780
aapb_109add5b4591_20140218112419, barcode 364585
aapb_109add5b4951_20140212101749, barcode 364586
aapb_109add589289_20140212103423, barcode 364587
aapb_109add589289_20140212103353, barcode 364588
aapb_109add5b4591_20140212101719, barcode 364589
aapb_0026b0f01ade_20140218105244, barcode 364593
aapb_0026b0f01ade_20140212103731, barcode 364594
aapb_0026b0f01ade_20140212103811, barcode 364595
aapb_0026b0f01ade_20140218103013, barcode 364596


2. Barcode drive

Upon being removed from its case, each drive is barcoded along the horizontal edge (to the left of the manufacturer’s barcode on the base of the drive) and placed in an anti-static bag – available on the shelves in the tech room – whenever it is not actively in use.   

For the drives that were not removed from their cases, the barcode was placed along the top of the drive box.  

3.  Identify LTO tape for drive

The script that generates metadata for the AAPB files requires that the ID of the LTO tape be entered in advance in order to accurately describe the file location in the instantiationLocation field.  Therefore, the destination LTO tape is always identified prior to running any operations on the drive. 

One LTO tape, uncompressed, can hold 2.5 TB of data.  Ideally, there should never put more than 2.45 TB on a single tape – nothing can be truly deleted off an LTO tape without erasing the entire tape and all its content, so it’s important to leave a margin of error in case files need to be altered or added.   Most of the drives used in the AAPB project can potentially hold 3 TB of material, so some of them may have too much content to be stored on one LTO tape.  In cases like this, I’ve usually transferred around 2.4 TB of data onto one LTO tape, and then the remaining 500 or 600 GB of data onto another AAPB-designated LTO tape that has available space on it.  In order to identify a destination tape for the overflow data, I check the document on the share drive titled ‘LTO_Tapes_Space’ to see how much available space exists on other AAPB LTO tapes.   The ‘LTO_Tapes_Space’ document is color-coded – red entries indicate that space is available, while green entries indicate that the tape is full.  Generally, tapes with less that 2 TB of data stored on them are considered to have available space.  


AAPB data should only go on a tape that already contains AAPB material, not a tape that contains material from other WGBH operations.   

Because the AAPB script automatically enters one LTO tape storage number as the instantiationLocation for all the files on a drive, I’ve created a find/change script that updates the XML metadata for any overflow files that are copied to a different LTO tape (as well as on the backup metadata stored on the share drive) with the correct instantiationLocation for those overflow files.  

If two drives each contain only a relatively small amount of data – less than 1.25 TB, or half of what can fit on an LTO tape – I have sometimes stored the contents of two complete drives on one LTO tape to save space.  In these cases, I determined the destination tape for a smaller drive by checking the size of the drive, and then looking at the ‘LTO_Tapes_Space’ list to see if the drive contents would fit on any of AAPB drives that have already been used.

4. Barcode and format LTO tape

LTO tapes are not barcoded on the tape itself, but on the horizontal edge of the plastic case.  The manufacturer’s barcode/serial number sticker on the front of the tape should always remain visible through the case; the institutional WGBH barcode is always placed on the side to the left of the manufacturer’s barcode.  

For instructions on formatting an LTO tape, see the document ‘LTO_Drive_Guide’ on the share drive. 

5. Generate instantiation documents and checksums for drive

In order to generate instantiation documents, it’s necessary to have the drive plugged into a computer, and an active connection to the WGBH MLA share drive.   It’s also necessary make sure the AA_PBCorescript_with_checks.sh script is executable on the computer performing the operation; this is done by typing ‘chmod +x’ into the command line, dragging and dropping the script into the command line window, and then pressing ENTER.

The first step is to manually create a folder on the share drive for the MD5 lists and instantiation documents that the script generates.  This is done by creating a new folder in the ‘MD5’ folder on the share drive, and then copying and pasting the name of the drive to be processed as it appears when plugged into your computer.   The new folder name formatted as barcode[xxxxxxx]_[drivename] –- for example, barcode123456_aapb_mydriveis_namedthis.

The next step is dragging and dropping the ‘aapb_PBCorescript_with_checks.sh’ script into the command line to run it.

The script prompts the user to enter a several pieces of information to allow it to accurately create the metadata for each file on the drive.  After providing each piece of information, hitting ENTER tells the program to move onto the next request.

“Drag and drop the drive to run the operation on"

Starting off simple, this asks the user to find the drive that they want to create metadata for in their Finder, and drop it into the command line.

“Drag and drop the directory where you would like to save your checksum files and instantiation records”

At this stage, the folder that was created on the share drive with the barcode name and the drive name should be dropped into the command line

“Drag and drop the CSV file containing the AAPB GUIDS”

This is one of the files specific to each batch of drives in the ‘AAPB_Batch_IDs’ folder on the share drive.  Since the majority of the drives are part of Batch 3, in most cases, you’ll want the ‘WGBH_batch3.csv’ document.  

“Type in the barcode number of the drive, in the format barcode######”

The user types in the designated drive barcode here – for example, barcode123456

“Type in the barcode number of the designated LTO tape, in the format barcode######”

This is the barcode of the LTO tape that was selected in the last step.   (Even if the drive appears to have more data than can be contained on one LTO tape, some of that data may include bad files or derivatives that won’t be backed up, so I’ve generally found it’s easier to assume one LTO tape and then edit if necessary than to try and run the script twice.)  The answer is entered in the same format as above, so, for example, barcode789123.  

Once all this information is entered, the script begins to run.  None of the documents required by the script – aapb_md5_total.csv, corrupted_files.csv, derivatives.csv, md5_original_values.csv, and your batch ID file – can be open while the script is running, or it interferes with the script’s ability to access them and run its data checks.   For a full 3 TB drive of data, the script may take up to five hours to run completely.   

After the script has completed running, the drive contains four folders: “bad_files,” “confirmed_good_files,” “derivatives,” and “files_needing_qc”.  

The bad_files folder contains files that have been proven via MD5 comparison to be corrupted.  They do not have XML metadata created for them and should not be copied to LTO tape.  The script automatically records the names and Artesia GUIDs of these files in the ‘corruped_files.csv’ document.

The derivatives folder contains files that are either exact MD5 matches of files that have already been ingested onto LTO tape, or can be judged from the filenames to be MP4 derivatives of master files that exist elsewhere in the repository.  They do not have XML metadata created for them and should not be copied to LTO tape. 

The confirmed_good_files folder contains files that have been proven via MD5 comparison to be unchanged from before they were uploaded into the Artesia DAM system, and therefore uncorrupted.  They should each have an XML metadata file living next to them in the folder, and should be copied to LTO tape.

The files_needing_qc folder contains files that are not derivatives, but don’t have an MD5 to compare against to prove that they are not corrupted.  They should each have an XML metadata file living next to them in the folder, and should be copied to LTO tape.  Further steps that should be taken on these files to facilitate later QC are documented below.

If any of these folders are empty by the time the script has finished running – for example, of all the files on a drive are proven to be good files, or none of the files turn out to be derivatives – I’ve made the decision to delete the empty folders for ease in future human-readability of the contents of the drive.  

The PBCore instantiation metadata generated by the script is named according to Artesia GUID (or folder name) and filename, in the format pbcore_instantiation_[ArtesiaGUID]_[filename].xml.  Each instantiation document lives in the folder next to the file that it describes, and should continue to do so any time the file is copied or transferred.   

The script also copies all the PBCore instantiation documents that it creates to a folder called ‘PBCoreInstantiations’ on the share drive, within the folder designated for the drive.  Additionally, it writes the MD5s for each file that it processes – so, every file that is not shown to be a bad file, a derivative, or a duplicate – to a document titled ‘barcode[xxxxxx]_MD5_Source.csv’ in the folder, as well as to the master MD5 file for the AAPB project, ‘aapb_md5_total.csv’.   

Finally, there are two simple QC procedures I’ve instituted to make sure that every metadata file includes an AAPB GUID, and to make sure that no bad or undreadable files are slipping through the cracks.  This involves two command line operations.

1. grep –L ‘cpb-aacip’ [select all the XML files copied to the PBCoreInstantaitions folder, drag and drop them into the command line, and then hit ENTER]

This searches the XML files created by the script for any file that doesn’t include the string ‘cpb-aacip,’ which serves as the prefix to every AAPB GUID.  

2. grep –L ‘instantiationEssenceTrack’  [select all the XML files copied to the PBCoreInstantaitions folder, drag and drop them into the command line, and then hit ENTER]

This searches the XML files created by the script for any file that doesn’t include the string ‘instantiationEssenceTrack,’ meaning the file had no track information that could be reported back by mediainfo, and was probably a failed file 

Once all these procedures have been completed, it’s time to move on to the next step.

NOTES: First, it’s important to recognize that the AAPB script makes the assumption that all AAPB drives are formatted the same way: as a set of top-level folders named after Artesia GUIDs, each containing a single file.  If, for some reason, a drive is not formatted in this way, the script will not function as designed, and some of the information it generates may be incorrect.  

Second, as noted in the project introduction, this script has consistently evolved over the course of the project.  The current and most complex form, which includes derivative checks, duplication checks, and comparison against known MD5s, was not implemented until processing was halfway completed on Batch 3-2.  Drives in Batch 1, 2, and 3-1, as well as the first few drives processed in Batch 3-2, therefore do not show the same file structure as the later drives.  

While I have always attempted to make the structure of the drives and the LTO tapes as human-readable as possible, the inconsistencies generated by the evolving scripts and workflows may lead to confusion.  The drives in Batches 1 and 2 – which contain no bad files -- do not have any sub-folders, and only appear as a set of folders named after Artesia GUIDs.  The drives in Batch 3-1, the first batch which includes failed files, vary.  Some of these drives are structured as a set of Artesia GUIDs with one separate folder, named bad_files, for files which have shown to be corrupted, while others are divided into two top-level folders, one named good_files and one named bad_files.  (The bad files in Batch 3-1 were identified by comparing the files on each drive against the original lists of corrupted files generated by Crawford.)   

Derivatives and duplicates on drives prior to Batch 3-2 are unfortunately not identified; because I didn’t realize how much of the material that we were processing fell into these categories during the early stages of the project, many of these lower-quality files slipped through the cracks.  

Additionally, due to a bug in the script during the processing of the drives in Batch 3-3, some of the files that were moved to the confirmed_good_files folder are incorrectly identified in the XML metadata as being located in the files_needing_qc folder.  In general, when using the XML metadata to identify the location of a file, the ID of the LTO tape will always be correct, but it’s possible that the exact folder structure may vary or be altered from what is recorded.

6. Create proofsheet for QC

This step is only required if files exist on the tape that cannot be automatically verified as good, bad, or derivative without the use of a human eye – in other words, if anything’s left in the ‘files_needing_qc’ folder after the AAPB script has finished running.   

In order to facilitate any future QC process and make it easier to identify failed files, we started using a utility called “QT_proofsheet” to generate sets of thumbnail images grabbed at regular time intervals from our unverified moving image files.

A script called ‘batch_qt_proofsheet.sh’ automatically generates proofsheets for every media file in the ‘files_needing_qc’ folder. 

The qt_proofsheet application needs to be installed on the computer where the operation is taking place in order for this script to work.  The application is part of a suite of programs called QT_Tools, available for installation from this site: http://omino.com/sw/qt_tools/

Although much simpler than the metadata generation script, the batch_qt_proofsheet script is similar in that it prompts the user for several pieces of information before executing its operation.  After providing each piece of information, hitting ENTER tells the program to move onto the next request.  

	“Drag and drop the drive”

As before, this is the drive that’s currently being processed.  

	“Drag and drop the directory containing the files needing QC”

This is the ‘files_needing_qc’ folder that was created by the AAPB script.   If there aren’t any files in the ‘files_needing_qc’ folder, there shouldn’t be a need to generate any proofsheets.   

	“Drag and drop the directory where you would like to save your proofs”

Once again, this is the folder that created on the share drive with the barcode and drive name of the drive being processed.    
The script automatically creates two folders with copies of the resulting proofsheets – one on the drive itself, and one in a folder called ‘QT_proofs’ in the drive’s folder on the share drive.   The proofsheets, like the XML metadata documents, are named according to Artesia GUID (or folder name) and file name, in the format proof_[ArtesiaGUID]_[filename].jpg.  

The proofsheets are designed to be easily viewed at a glance to determine whether a file was clearly playable all the way through, or shows signs of errors that might indicate corruption, significantly cutting down the number of files that need to be manually checked.  

However, while QT_proofsheet is a valuable tool, it has a few key weaknesses – primarily, the fact that it cannot generate an accurate proofsheet for a file that is not based in a Quicktime codec.  While the majority of the AAPB files are formatted either as Quicktime or h.264 moving image files, the set of files to be processed does include some AVI, DV, and camera raw files that QT_proofsheet does not know how to process, hence the addition of the next step in the workflow.   

7.  List files not proofed

In order to make sure that future QC testers are aware of the files that don’t have proofsheets created for them, I wrote a script called ‘proof_check.sh’ that generates a list of files that could not be proofed by qt_proofsheet.   It operates in a very similar fashion to the batch_qt_proofsheet script, with the same set of input instructions:

	“Drag and drop the drive”

This is the drive that’s currently being processed.  

	“Drag and drop the directory containing the files needing QC”

This is the ‘files_needing_qc’ folder that was created by the AAPB script.   If there aren’t any files in the ‘files_needing_qc’ folder, there shouldn’t be a need to generate any proofsheets.   

	“Drag and drop the directory where you would like to save your proofs”

Once again, this is the  folder created on the share drive with the barcode name and the drive name.   (Note: this should not be the QT_proofs folder that was created by the last operation, but instead the higher-level folder for the drive.)

Once this operation has run, a file will be created in both the QT_proofs folder on the hard drive and the QT_proofs folder in the share drive that reads, “The following files are incompatible with qt_proofsheet; please check file manually,” followed by a list of the problematic files.

A QC tester should be able to briefly scan the proofsheets to verify whether files are clearly successful or potentially demonstrate errors, then refer back to the files_not_proofed list to check what files still need to be manually QCed to ensure that they aren’t corrupt – still a time-consuming process, but much less so than if every single file had to be checked by hand.  

After all the files have been fully documented as much as we have the tools to document them, the next step is to actually back them up.  

8. Copy files and instantiation documents onto LTO 

For each drive, a top-level folder is created on the LTO tape named after the drive being transferred.

I’ve used the rsync transfer protocol to move data from hard drives to LTO – I’ve found it to be fast, reliable, and convenient in how it tracks the progress of ongoing transfers.  It also makes it easy to transfer additional or updated data if errors occur in the transfer process without accidentally duplicating any files that have already been transferred.  (Although ideally data should not be altered or edited once it’s been written to LTO tape – as any ‘overwritten’ data still exists as a hidden file which takes up space on the tape overall – I have occasionally broken this rule in order to fix mistakes in the XML metadata, on the principle that inaccurate metadata may cause a bigger problem long-term than 27 KB of hidden data on the tape.)  

When using rsync, the syntax is: rsync [command options] [all folders/files to be transferred] [destination folder]

The rsync command options I’ve been using are –rtv and --progress

-r: instructs rsync to transfer all folders recursively (which means the contents of the folder, including subfolders, and the contents of the subfolders, etc.)
-t: instructs rsync to preserve timecodes while transferring, instead of updating them to reflect the transfer date
-v: instructs rsync to be verbose as it transfers, providing information throughout the course of the process and a summary at the end 
--progress: instructs rsync to show the progress of the transfer as it occurs, including what percentage of the file has already been downloaded, the rate of the transfer, and an estimate of how much time is left before the transfer is completed

Therefore, the procedure to transfer the files starts by opening the command line, and type:

rsync –rtv –-progress

into the window; dragging and dropping the folders to be transferred (normally this would be the confirmed_good_files folder and the files_needing_qc folder) into the command line; and then, lastly, dragging and dropping the destination folder (the folder for the drive on the LTO tape) before hitting ENTER.  

 

It should take 4-5 hours to transfer 2.5 TB of material, or enough to fill an entire LTO tape.

Because rsync will automatically check the destination folder and verify filenames, timestamps and filesizes to see if files that are slated to be transferred already exist there in their current form – and will only transfer data that either does not exist in the destination folder, or that has been updated since being transferred – I usually run the same rsync command twice, just to make sure that all files transferred correctly.  (When transferring complex hard drive structures, I have noticed that some files occasionally do not transfer fully the first time.  Although I’ve never had this problem with an AAPB drive so far, it’s a relatively simple extra precaution to take.)

Hitting the ‘up’ arrow while in the command line calls up the previous command that was run, so in order to run the rsync command again, the easiest way is to hit the ‘up’ arrow and then hit ENTER.   Most of the time, the result of this second pass will be a very quick operation in which rsync reports that a very small number of bytes (generally 20) were transferred.  

NOTE: For the first few tapes that I transferred, I had not yet developed the protocol of creating a top-level folder named after the drive on the LTO tape.  The earliest tapes therefore have all the Artesia GUID folders on the top level, rather than one single top-level folder named for the drive.  In some cases, when one of these earlier LTO tape contains files from more than one drive, there may be a number of top-level Artesia GUID folders and one top-level drive folder.  

9. Generate checksums for LTO

Once the files have been transferred, it’s important to check and make sure that all the data did in fact transfer successfully.  For this, I created the AA_LTO_checksum_update script.  Again, the process for making the script executable on the computer is typing ‘chmod +x’ into the command line, dragging and dropping the script, and hitting ENTER.

Although much simpler than the metadata generation script, the checksum script is once again similar in that it prompts the user for several pieces of information before executing its operation.  After providing each piece of information, the user hits ENTER to tell the program to move onto the next request.  

“Drag and drop the directory where you would like to save your checksum files”

This is the same folder on the share drive named with the barcode and drive ID, in which the source MD5s and PBCore instantiation documents for the drive are already saved.  
	
“Type in the barcode number of the drive, in the format barcode######”

This information is entered in the same way as it was entered for the metadata generation script.

	“Drag and drop the directory for the drive on the LTO tape”

This is the directory that was created in the last step, named after the drive, which contains all of the transferred data for that drive.

The script then automatically runs run a checksum on every content file in the folder, and write those MD5s to a file in the share drive named ‘barcode[xxxxxx]_MD5_ Copy.csv’.  This process will probably take slightly less time than the 4-5 hours required for the transfer process.  

If the drive contains enough data that it cannot fit on a single LTO tape, a second script, ‘AA_LTO_checksum_second_tape.sh’, is used to generate checksums from the files transferred to a second tape.  The script works exactly the same way as the ‘AA_LTO_checksum_update.sh’ script, except that the file that it creates is named ‘barcode[xxxxxx]_MD5_Copy_2.csv’.

10. Compare checksums to verify successful transfer

When all the files have been copied onto a single tape, this checksum comparison is accomplished with a simple ‘diff’ operation in the command line, comparing the MD5_Source and the MD5_Copy files against each other to ensure that all the files transferred have the same MD5 on the LTO as they did on the hard drive.

The first step is to change directories to work within the folder created for the drive on the share drive: 

cd [drag and drop the folder] 

The command I’ve been using to run the comparison is:

diff <(sort [drag and drop the ‘barcode[xxxxxx]_MD5_Source.csv’ file]) <(sort [drag and drop the ‘barcode[xxxxxx]_MD5_Copy.csv’ file] > diff.txt

To break this down a little: the ‘diff’ looks for differences between the two files; the ‘sort’ command makes sure that both files are sorted alphabetically so that the ‘diff’ command doesn’t report back on differences in the position of the entries in the spreadsheets that don’t reflect genuine differences in filenames and MD5s; and the output then gets written to a file called ‘diff.txt’ in the folder for the drive on the share drive.  

This command creates a ‘diff.txt’ file in the folder for the drive on the share drive.  Ideally, the file is blank – meaning that the two files are exactly the same.  If it isn’t blank, that’s a sign that something is wrong, and a file didn’t transfer properly.

The diff operation – like everything else -- becomes a little more complicated when one drive, with one MD5_Source document, is copied onto two different LTO tapes.  This means that there are two different MD5_Copy documents, MD5_Copy and MD5_Copy_2.  Since neither of these documents contains all the files and MD5s listed in the MD5_Source, trying to run a diff comparison on either one will throw up a lot of false indications of error.  In these cases, what I’ve generally been doing is merging MD5_Copy and MD5_Copy_2 to make a third document, MD5_Copy_Merged, which can be easily compared against the MD5_Source document.

The easiest way to combine two documents in the command line is to use the ‘cat’ command:

cat [drag and drop the ‘barcode[xxxxxx]_MD5_Copy.csv’ file] [drag and drop the ‘barcode[xxxxxx]_MD5_Copy_2.csv’ file] > barcode[xxxxxx]_MD5_Copy_Merged.csv

Once the MD5_Copy_Merged file has been created, it can be used in the ‘diff’ procedure outlined above.  

For the record, to date (4/16/2015, having processed 41/70 drives), I have only once come across a genuine MD5 mismatch due to a file transfer error. 

11. Document LTO and checksum locations in LTO spreadsheet

This step in the workflow actually encompasses two documents.  The first one – and the easiest one – is the ‘LTO_Tapes_Space.rtf’ document on the share drive.  Every time a tape is used, an entry is created for it in this file, in the format:

LTO [Serial Number] barcode[xxxxxx] – [TB USED] (AA DRIVE, NOT BACKED UP)

If the tape is full, the text of the entry is green.  If the tape still has room that can be used for other folders, the text of the entry is red.   When more data is added to a tape, the entry is updated to reflect the amount of data used on the tape.

The method I’ve been using to report the amount of data used on the tape is to navigate to the folder on the LTO tape for the drive that was just transferred and press Command-I to open the Get Info window, which displays the size of the folder.  (It may take a few moments to calculate the size before the actual size appears.)  The size can be noted in either gigabytes (GB) or terabytes (TB).  

NOTE: I’ve found that the size information displayed in the ‘Get Info’ for an LTO tape mounted on a Mac computer is often inconsistent and inaccurate.   If there are two folders on the tape, it is definitely more accurate to get the size information for each folder and then add the sizes of the two folders together, rather than trying to get size information for the whole tape.  

After the ‘LTO_Tapes_Space.rtf’ document has been updated, the next step is to update the ‘Drive_LTO_Data.xlsx’ file.  This file, also located in the share drive, is currently the master record for all the data on all of the WGBH LTO tapes.  It’s an Excel spreadsheet with 13 columns: Copy_Date, Original_Drive_Name, Original_Drive_Barcode, Folder_Copied, Original_Drive_Folder_Source_MD5, Preservation_LTO_Tape_Barcode, Preservation_LTO_Tape_Serial_Number, Preservation_LTO_Copy_MD5, Off_Site_LTO_Barcode, Off_Site_LTO_Serial_Number, Off_Site_LTO_Backup_MD5, Size, In_Hydra_Dam?, Notes, and Mars Asset Record Created?

Each transfer procedure equates to one row in the file.  Note that this means that if an AAPB drive is large enough that overflow files are transferred to a second LTO tape, there will be two rows in the file – one for the first transfer process, and one for the second.  

Copy_Date: This is the date that the files were transferred to LTO.

Original_Drive_Name: This is the unique name of the drive (i.e. aapb_mydriveis_namedthis)

Original_Drive_Barcode: This is the barcode that has been assigned to the drive.

Folder_Copied: For the AAPB drives, this is usually either filled out as ‘ALL,’ or ‘all except failed files and derivatives.’  When there are overflow files, I have usually noted this as ‘all except files copied to barcode[xxxxxx],’ and then on the line for the second transfer procedure independently noted all the folders (by AAPB GUID) that were copied to the second LTO tape.

Original_Drive_Folder_Source_MD5: This is the file path on the share drive that leads to the ‘barcode[xxxxxx]_MD5_Source.csv’ document.  The file path is copied from the ‘File Info’ window for that documented, where it’s listed under the ‘server’ section of the window, and then pasted into the Excel document.  

A sample file path looks like this: smb://deptfs.wgbh.org/dept/MLA/Archives/LTO_Tape_Preservation/MD5/barcode365002_aapb_109add589289_20141211133657/barcode365002_MD5_Source.csv

Note that the address starts with smb:// -- this indicates the sever message block protocol that provides access to the department share folder.   (The other address shown in the  ‘File Info’ window, which starts with /Volumes/, is a local address that is only applicable when a Mac computer is logged into the share folder; the smb:// address is a global address, and can be used at any time to log into the share folder and navigate to the specific file.)  

Preservation_LTO_Tape_Barcode: This is the barcode assigned to the tape to which the files have been transferred.

Preservation_LTO_Tape_Serial_Number: This is the manufacturer’s serial number found in the sticker on the front of the tape.  

Preservation_LTO_Copy_MD5: Similarly to the Original_Drive_Folder_Source_MD5, this is the file path on the share drive that leads to the ‘barcode[xxxxxx]_MD5_Copy.csv’ document  (or Copy_2, if this is an entry for the overflow files on a tape).  In other words, the path looks exactly the same as the path for the Original_Drive_Folder_Source_MD5, except that the filename of the document will be ‘barcode[xxxxxx]_MD5_Copy’ instead of ‘barcode[xxxxxx]_MD5_Source.’

Off_Site_LTO_Barcode: Generally, WGBH LTO tapes are backed up on a second duplicate LTO tape, which is kept offsite in Iron Mountain.  However, this is not the case for AAPB LTO tapes; since the material on these tapes is all preserved offsite at the Library of Congress as part of the AAPB collection, we decided it wasn’t necessary to create an additional offsite tape.  Therefore, for AAPB material, this field should be marked N/A.

Off_Site_LTO_Serial_Number: Again, this should be N/A.

Off_Site_LTO_Backup_MD5: And once again, N/A.

Size: This field contains information about the amount of data that was transferred to the LTO tape. 

In_Hydra Dam?: HydraDam is designed to track digital asset metadata and may one day (ideally) replace the Drive_LTO_Data spreadsheet; however, since HydraDam is not yet operational at WGBH, this field is currently blank for all entries in the Excel sheet.  This may change when HydraDam becomes integrated into regular WGBH workflows.

Notes: Any relevant information that can’t be conveyed by the other fields should go here. 

Mars Asset Record Created?: MARS is the Media Archive Research System at WGBH.  Creating the MARS asset record is one of the last steps of the workflow, so this field will not be filled out until after the final steps have been completed.  

12. Transfer drive to vault

The AAPB drives are stored in anti-static bags in CD Case 5, on their sides with the barcode facing up.  The CD Case drawers have dividers and bookends to help keep their contents organized; we have determined that four drives stored on their end equal approximately the width of one bookend, and have placed our dividers accordingly.   The drives are organized in ascending order of barcode.

The twelve drives that could not be removed from their cases are not stored in CD Case 5, but instead on side 45, bay 4, shelf 6 in the vault.  

13. Transfer LTO to vault

All LTO tapes – for the AAPB as well as for any other WGBH operations – are stored on side 141, bay 4 in the vault on shelves 3 and 4, organized approximately in ascending order of barcode.

NOTE:  As a general practice, I’ve been keeping LTO tapes that have less than 2 TB of data on them at the LTO station, in case I need a tape with extra space available for overflow files from a large drive.  Once a tape has over 2 TB of data on it, I consider it full and move it to the vault.  At the time of the writing of this documentation, any tape that shows as green in the ‘LTO_Tapes_Space.rtf’ document on the share drive should be in the vault; any tape that shows as red has not yet been moved into the vault, because it still has space available.  At the conclusion of the project, all the AAPB tapes will be transferred to the vault whether or not they’re completely filled.  

14. Create record for drive

Asset records for all physical material held by WGBH are created in the ‘Assets’ section of MARS, the filemaker database used to track WGBH’s archival holding.   There are several layout options through which to access the ‘Asset’ records section of MARS; the key one in this case is ‘Asset Inventory,’ which displays location information for the item.  

These are the key fields that should be filled out for every AAPB drive:

Tape #: This is the unique name of the drive as it appears when plugged into a computer, i.e. aapb_mydriveis_namedthis

Item Type: For all container formats holding digital assets, this should be filled in as ‘Backup (computer files) 

Format: For all drives, this should be filled in as ‘Hard Drive’

Barcode: This field should only contain the six numbers of the barcode, i.e. 123456

Location: For all assets, this should be entered as ‘Vault’

Order #: This field autofills with a unique record ID any time a new record is created in MARS, and should never be changed

Creator: This will be the first and last name of the archivist creating the record

Dept: For AAPB drives, the originating department is always ‘Media Library and Archives’

Date Entered: This field autofills with the date of record entry when the record is created, and should not be changed

ID_MARS: This field also autofills a distinct unique MARS ID any time a new record is created, and should never be changed

NOTES_ACCESS_RESTRICTIONS: All AAPB drives will be restricted as ‘Viewing Permission Required’

Subject/Description: The standard language to be included in this field is as follows:

“American Archive returned drive from Crawford, born digital export from Artesia, backed up on LTO 6 tape [xxxxxx]”

If there are overflow files, the language is as follows:

“American Archive returned drive from Crawford, born digital export from Artesia, backed up on LTO 6 tapes [xxxxxx] and [xxxxxx]

For the majority of the drives, which are all located in the same place, the specific asset location information in the bottom section of the record will be filled out as follows:

SHELF: CD Case

DRAWER: 5

Vault Shelf BC: ML004966

The fields for ‘Side’ and ‘Bay’ should be blank.

For the drives which could not be removed from their cases, the asset location information is as follows:

SIDE: 45

BAY: 4

SHELF: 6

Vault Shelf BC: ML008194

Once the record for the LTO tape is created in MARS, the Drive_LTO_Data spreadsheet should be updated accordingly to indicate that the tape has been documented and stored appropriately in the vault.  

This is the final stage in the drive archiving workflow developed for this project.  A future project will integrate the records created during this workflow into the DAM system currently in development and link the item-level technical metadata generated by the scripts with the content metadata already in MARS.  

Appendix A: Scripts Required for the Ingest Process:

AA_PBCorescript_with_checks.sh (smb://deptfs.wgbh.org/dept/MLA/Archives/LTO_Tape_Preservation/scripts/AA_PBCorescript_with_checks.sh)

A script for batch generating metadata about the files on each AAPB drive, eliminating duplicates and known bad files.  

- automatically restructures the drive to create folders for verified files, bad files, derivatives
- compares the filename of each file against a list of mp4 derivatives of high-quality files that should not be included in the archive; if it matches, moves the file to the ‘derivatives’ folder
- compares the MD5 checksum of each file against the list of MD5 checksums from all previously transferred files; if it proves to be an exact duplicate, moves the file to the ‘derivatives’ folder
- compares the filename of each file against the list of files with pre-existing checksums, and then compares the MD5 checksum to see if they match; if so, moves the file to the ‘confirmed_good_files’ folder; if they do not match, moves the file to the ‘bad_files’ folder, and writes the drive, Artesia GUID, and filename to a document named ‘corrupted_files.csv’ in the AA_Batch_IDs folder on the share drive.  
- writes the MD5 checksum of each file to a document titled ‘barcode[xxxxxx]_MD5_Source.csv’, located in the drive’s folder in the ‘MD5’ section of the share drive
- copies the MD5 checksum of each file to a document titled ‘aapb_MD5_total.csv’, which collects the MD5 checksums for all AAPB files for the purpose of running a duplication check
- compares the Artesia GUID folder ID against a csv containing Artesia GUIDs and AAPB GUIDs to find the accurate AAPB GUID for each file
- creates a PBCore Instantiation document for each file that lists the AAPB GUID as the instantiationIdentifier, the filepath on the designated LTO as the instantiationLocation, and the AAPB GUID, the filename, the original drive ID, and the MD5 checksum as instantiationAnnotations, with the rest of the metadata generated from MediaInfo
- copies the PBCore Instantiation document to the folder containing the file, and to a subfolder called ‘PBCoreInstantiations’ within the drive’s folder in the ‘MD5’ section of the share drive

AA_PBCorescript2.sh
(smb://deptfs.wgbh.org/dept/MLA/Archives/LTO_Tape_Preservation/scripts/AA_PBCorescript2.sh)

A version of the above script which does not include the automatic checks for derivatives, duplicates, and bad files, and which can be run on a subfolder instead of a whole drive. 
  
AA_LTO_checksum_update.sh
(smb://deptfs.wgbh.org/dept/MLA/Archives/LTO_Tape_Preservation/scripts/AA_LTO_checksum_update.sh)

A script which generates a checksum for each file in a folder and copies it to a document titled ‘barcode[xxxxxx]_MD5_Copy.csv’, located in the drive’s folder in the ‘MD5’ section of the share drive.  Used for comparing the checksums of files copied to LTO against their original checksums as recorded when on the drive to verify complete and successful transfers.  (AA_LTO_checksum.sh is the original version of the script, which was used for the first portion of the project; AA_LTO_checksum_update modifies it in a few small particulars to make it easier to use and less reliant on the file structure of the LTO tape.)  

AA_LTO_checksum_second_tape.sh
(smb://deptfs.wgbh.org/dept/MLA/Archives/LTO_Tape_Preservation/scripts/AA_LTO_checksum_second_tape.sh)

Essentially the same as the AA_LTO_checksum script in almost every respect, except that it copies the checksums into a document titled ‘barcode[xxxxxx]_MD5_Copy_2.csv.’  Designed to be used with drives that contain more than 2.5 TB of data, requiring some of the files to be copied onto a second LTO.    

batch_qt_proofsheet.sh
(smb://deptfs.wgbh.org/dept/MLA/Archives/LTO_Tape_Preservation/scripts/batch_qt_proofsheet.sh)

A script which runs a program called ‘qt_proofsheet’ on each file in a directory to generate a proof sheet made up of a collection of thumbnail images taken from the file at regular timed intervals, then copies the proofs to a folder called ‘QT_proofsheet’ on the original drive and in the drive’s folder in the ‘MD5’ section of the share drive.

proof_check.sh
(smb://deptfs.wgbh.org/dept/MLA/Archives/LTO_Tape_Preservation/scripts/proof_check.sh) 

A script which creates a document identifying the files on a folder that do not have existing proofsheets (generally due to format incompatibilities with qt_proofsheet)


Appendix B: Documents Required for the Ingest Process:

aapb_md5_total.csv
(smb://deptfs.wgbh.org/dept/MLA/Archives/LTO_Tape_Preservation/MD5/aapb_md5_total.csv)

List of all files transferred over the course of the project with their associated MD5s

corrupted_files.csv
(smb://deptfs.wgbh.org/dept/MLA/Archives/LTO_Tape_Preservation/AA_Batch_IDs/corrupted_files.csv)

List of all the files that did not pass MD5 checksum validation and should therefore be included in the list of failed files to be recovered from Artesia

derivatives.csv
(smb://deptfs.wgbh.org/dept/MLA/Archives/LTO_Tape_Preservation/AA_Batch_IDs/derivatives.csv)

List of MP4 derivative files to be removed from inclusion in the repository

Drive_LTO_Data.xls
(smb://deptfs.wgbh.org/dept/MLA/Archives/LTO_Tape_Preservation/Drive_LTO_Data.xls)

Excel document containing information about drives copied to LTO, including the drive name and barcode, the LTO number and barcode, the size of the drive contents copied, and the location of the MD5 checksum data for comparison

LTO_tapes_space.rtf
(smb://deptfs.wgbh.org/dept/MLA/Archives/LTO_Tape_Preservation/LTO_Tapes_Space.rtf)

List of LTO tapes used in the WGBH MLA, including the tape number, the tape barcode, and the amount of space used on the tape

md5_original_values.csv
(smb://deptfs.wgbh.org/dept/MLA/Archives/LTO_Tape_Preservation/AA_Batch_IDs/md5_original_values.csv)

List of all the files with documented MD5s from before they went into Artesia, and the associated MD5s

WGBH_Batch1_LimitedCSV_final.csv
(smb://deptfs.wgbh.org/dept/MLA/Archives/LTO_Tape_Preservation/AA_Batch_IDs/WGBH_Batch1_LimitedCSV_final.csv)

WGBH-Batch2-140211.csv
(smb://deptfs.wgbh.org/dept/MLA/Archives/LTO_Tape_Preservation/AA_Batch_IDs/WGBH-Batch2-140211.csv)

WGBH_batch3.csv
(smb://deptfs.wgbh.org/dept/MLA/Archives/LTO_Tape_Preservation/AA_Batch_IDs/WGBH_batch3.csv)

WGBH-Batch4-LimitedCSV.csv
(smb://deptfs.wgbh.org/dept/MLA/Archives/LTO_Tape_Preservation/AA_Batch_IDs/WGBH-Batch4-LimitedCSV.csv)

CSV documents containing the Artesia GUID and the AAPB GUID for the files in each batch, allowing the AAPB GUID to be automatically inserted in to the PBCore Instantiation document



